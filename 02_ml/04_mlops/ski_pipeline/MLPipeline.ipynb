{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# [Custom Estimators](https://scikit-learn.org/stable/developers/develop.html)\n",
    "\n",
    "## get_params and set_params\n",
    "\n",
    "All scikit-learn estimators have `get_params` and `set_params` functions. The get_params function takes no arguments and \n",
    "returns a dict of the `__init__` parameters of the estimator, together with their values.\n",
    "\n",
    "It must take one keyword argument, deep, which receives a boolean value that determines whether the method should \n",
    "return the parameters of sub-estimators (for most estimators, this can be ignored). \n",
    "\n",
    "The default value for deep should be True. For instance considering the following estimator:\n",
    "\n",
    "The parameter deep will control whether or not the parameters of the subsestimator should be reported. Thus when deep=True, the output will be:\n",
    "\n",
    "The easiest way to implement these functions, and to get a sensible `__repr__` method, is to inherit from `sklearn.base.BaseEstimator`. If you do not want to make your code dependent on `scikit-learn`, the easiest way to \n",
    "implement the interface is"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "my_extra_param -> random\n",
      "subestimator__C -> 1.0\n",
      "subestimator__class_weight -> None\n",
      "subestimator__dual -> False\n",
      "subestimator__fit_intercept -> True\n",
      "subestimator__intercept_scaling -> 1\n",
      "subestimator__l1_ratio -> None\n",
      "subestimator__max_iter -> 100\n",
      "subestimator__multi_class -> auto\n",
      "subestimator__n_jobs -> None\n",
      "subestimator__penalty -> l2\n",
      "subestimator__random_state -> None\n",
      "subestimator__solver -> lbfgs\n",
      "subestimator__tol -> 0.0001\n",
      "subestimator__verbose -> 0\n",
      "subestimator__warm_start -> False\n",
      "subestimator -> LogisticRegression()\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "class MyEstimator(BaseEstimator):\n",
    "    def __init__(self, subestimator=None, my_extra_param=\"random\"):\n",
    "        self.subestimator = subestimator\n",
    "        self.my_extra_param = my_extra_param\n",
    "        \n",
    "my_estimator = MyEstimator(subestimator=LogisticRegression())\n",
    "for param, value in my_estimator.get_params(deep=True).items():  # returns a dictionary\n",
    "    print(f\"{param} -> {value}\")\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for param, value in my_estimator.get_params(deep=False).items():  # returns a dictionary\n",
    "    print(f\"{param} -> {value}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "my_extra_param -> random\n",
      "subestimator -> LogisticRegression()\n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The easiest way to implement these functions, and to get a sensible `__repr__` method, is to inherit \n",
    "from `sklearn.base.BaseEstimator`. If you do not want to make your code dependent on `scikit-learn`, the easiest way to \n",
    "implement the interface is\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        # suppose this estimator has parameters \"alpha\" and \"recursive\"\n",
    "        return {\"alpha\": self.alpha, \"recursive\": self.recursive}\n",
    "    \n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "        \n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cloning \n",
    "\n",
    "For use with the `model_selection` module, an estimator must support the `base.clone` function to replicate an estimator. \n",
    "This can be done by providing a get_params method. If get_params is present, then `clone(estimator)` will be an instance \n",
    "of `type(estimator)` on which set_params has been called with clones of the result of `estimator.get_params()`.\n",
    "\n",
    "Objects that do not provide this method will be deep-copied (using the Python standard function copy.deepcopy) \n",
    "if `safe=False` is passed to clone."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pipeline\n",
    "\n",
    "## Creating a custom estimator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import euclidean_distances\n",
    "class TemplateClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, demo_param='demo'):\n",
    "        self.demo_param = demo_param\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "        # Store the classes seen during fit\n",
    "        self.classes_ = unique_labels(y)\n",
    "\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "        # Return the classifier\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        # Check is fit had been called\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "\n",
    "        closest = np.argmin(euclidean_distances(X, self.X_), axis=1)\n",
    "        return self.y_[closest]\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"Fit the model with X and apply the dimensionality reduction on X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "\n",
    "        y : Ignored\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : ndarray of shape (n_samples, n_components)\n",
    "            Transformed values.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        This method returns a Fortran-ordered array. To convert it to a\n",
    "        C-ordered array, use 'np.ascontiguousarray'.\n",
    "        \"\"\"\n",
    "        return X\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Apply dimensionality reduction to X.\n",
    "\n",
    "        X is projected on the first principal components previously extracted\n",
    "        from a training set.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            New data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : array-like, shape (n_samples, n_components)\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "\n",
    "        >>> import numpy as np\n",
    "        >>> from sklearn.decomposition import IncrementalPCA\n",
    "        >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "        >>> ipca = IncrementalPCA(n_components=2, batch_size=3)\n",
    "        >>> ipca.fit(X)\n",
    "        IncrementalPCA(batch_size=3, n_components=2)\n",
    "        >>> ipca.transform(X) # doctest: +SKIP\n",
    "        \"\"\"\n",
    "        return X\n",
    "        \n",
    "        \n",
    "CLFEX = TemplateClassifier()\n",
    "# check_estimator(TemplateClassifier(CLFEX))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating a pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "Pipeline(steps=[('clfex', TemplateClassifier()), ('reduce_dim', PCA()),\n                ('clf', SVC())])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 32
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "estimators = [('clfex', TemplateClassifier()), ('reduce_dim', PCA()), ('clf', SVC()), ]\n",
    "pipe = Pipeline(estimators)\n",
    "pipe"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1., 1., 1.],\n       [1., 1., 1.],\n       [1., 1., 1.]])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 24
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "pipe.get_params()\n",
    "pipe.fit(np.ones((3, 3)), np.ones(3))\n",
    "# pipe.set_params(clf__C=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}